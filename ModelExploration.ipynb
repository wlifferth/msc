{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>intensity</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296909</th>\n",
       "      <td>She wears her school jacket open.</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.2938973, 0.02742757, -0.080566145, 0.054517...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204220</th>\n",
       "      <td>Highly recommended read.In a quite elegant, su...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.04066607, 0.19786465, -0.16860504, -0.0083...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202973</th>\n",
       "      <td>The faculties of design are best suited to dra...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.074057005, 0.081415884, -0.049602706, -0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56697</th>\n",
       "      <td>But, unfortunately, I ordered this item and ne...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.004936785, 0.24104582, -0.12420635, -0.125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32884</th>\n",
       "      <td>Well thats it... Peace Out!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.02340915, 0.256462, -0.11517024, -0.132226...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  polarity  \\\n",
       "296909                  She wears her school jacket open.       0.5   \n",
       "204220  Highly recommended read.In a quite elegant, su...       1.0   \n",
       "202973  The faculties of design are best suited to dra...       0.5   \n",
       "56697   But, unfortunately, I ordered this item and ne...       0.0   \n",
       "32884                         Well thats it... Peace Out!       0.0   \n",
       "\n",
       "        intensity                                             vector  \n",
       "296909        0.0  [0.2938973, 0.02742757, -0.080566145, 0.054517...  \n",
       "204220        1.0  [-0.04066607, 0.19786465, -0.16860504, -0.0083...  \n",
       "202973        0.0  [0.074057005, 0.081415884, -0.049602706, -0.12...  \n",
       "56697         1.0  [-0.004936785, 0.24104582, -0.12420635, -0.125...  \n",
       "32884         1.0  [-0.02340915, 0.256462, -0.11517024, -0.132226...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.read_hdf('data/training_data.hdf', 'id')\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5    0.5006\n",
       "0.0    0.2538\n",
       "1.0    0.2456\n",
       "Name: polarity, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df['polarity'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['int_high'] = training_df['intensity'].apply(lambda x: 1 if x > 0.6 else 0)\n",
    "training_df['int_med'] = training_df['intensity'].apply(lambda x: 1 if (x < 0.6 and x > 0.4) else 0)\n",
    "training_df['int_low'] = training_df['intensity'].apply(lambda x: 1 if x < 0.4 else 0)\n",
    "training_df['pol_pos'] = training_df['polarity'].apply(lambda x: 1 if x > 0.6 else 0)\n",
    "training_df['pol_neu'] = training_df['polarity'].apply(lambda x: 1 if (x < 0.6 and x > 0.4) else 0)\n",
    "training_df['pol_neg'] = training_df['polarity'].apply(lambda x: 1 if x < 0.4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.6664\n",
      "1    0.3336\n",
      "Name: int_high, dtype: float64\n",
      "0    0.9382\n",
      "1    0.0618\n",
      "Name: int_med, dtype: float64\n",
      "1    0.6046\n",
      "0    0.3954\n",
      "Name: int_low, dtype: float64\n",
      "0    0.7544\n",
      "1    0.2456\n",
      "Name: pol_pos, dtype: float64\n",
      "1    0.5006\n",
      "0    0.4994\n",
      "Name: pol_neu, dtype: float64\n",
      "0    0.7462\n",
      "1    0.2538\n",
      "Name: pol_neg, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(training_df['int_high'].value_counts(normalize=True))\n",
    "print(training_df['int_med'].value_counts(normalize=True))\n",
    "print(training_df['int_low'].value_counts(normalize=True))\n",
    "print(training_df['pol_pos'].value_counts(normalize=True))\n",
    "print(training_df['pol_neu'].value_counts(normalize=True))\n",
    "print(training_df['pol_neg'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df['vector'].values[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(300, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(32, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(6, activation=tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = np.stack(training_df['vector'].values, axis=0)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data = training_df[['int_high', 'int_med', 'int_low', 'pol_pos', 'pol_neu', 'pol_neg']].values\n",
    "label_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.2\n",
    "cutoff = int(len(input_data) * split_frac)\n",
    "x_val = input_data[:cutoff]\n",
    "x_train = input_data[cutoff:]\n",
    "\n",
    "y_val = label_data[:cutoff]\n",
    "y_train = label_data[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/80\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 0.6509 - acc: 0.6524 - val_loss: 0.5870 - val_acc: 0.7554\n",
      "Epoch 2/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.5544 - acc: 0.7494 - val_loss: 0.5187 - val_acc: 0.7502\n",
      "Epoch 3/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.5022 - acc: 0.7570 - val_loss: 0.4773 - val_acc: 0.7808\n",
      "Epoch 4/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4640 - acc: 0.7808 - val_loss: 0.4494 - val_acc: 0.7841\n",
      "Epoch 5/80\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.4396 - acc: 0.7872 - val_loss: 0.4301 - val_acc: 0.7932\n",
      "Epoch 6/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4185 - acc: 0.7977 - val_loss: 0.4138 - val_acc: 0.8026\n",
      "Epoch 7/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4012 - acc: 0.8095 - val_loss: 0.4008 - val_acc: 0.8114\n",
      "Epoch 8/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3868 - acc: 0.8172 - val_loss: 0.3916 - val_acc: 0.8174\n",
      "Epoch 9/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3759 - acc: 0.8245 - val_loss: 0.3851 - val_acc: 0.8197\n",
      "Epoch 10/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3681 - acc: 0.8280 - val_loss: 0.3805 - val_acc: 0.8217\n",
      "Epoch 11/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3618 - acc: 0.8312 - val_loss: 0.3785 - val_acc: 0.8222\n",
      "Epoch 12/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3576 - acc: 0.8344 - val_loss: 0.3770 - val_acc: 0.8207\n",
      "Epoch 13/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3534 - acc: 0.8358 - val_loss: 0.3754 - val_acc: 0.8200\n",
      "Epoch 14/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3510 - acc: 0.8380 - val_loss: 0.3755 - val_acc: 0.8195\n",
      "Epoch 15/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3467 - acc: 0.8399 - val_loss: 0.3688 - val_acc: 0.8249\n",
      "Epoch 16/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3435 - acc: 0.8417 - val_loss: 0.3670 - val_acc: 0.8253\n",
      "Epoch 17/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3391 - acc: 0.8435 - val_loss: 0.3654 - val_acc: 0.8258\n",
      "Epoch 18/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3363 - acc: 0.8448 - val_loss: 0.3649 - val_acc: 0.8248\n",
      "Epoch 19/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3334 - acc: 0.8465 - val_loss: 0.3644 - val_acc: 0.8255\n",
      "Epoch 20/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3305 - acc: 0.8479 - val_loss: 0.3649 - val_acc: 0.8262\n",
      "Epoch 21/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3277 - acc: 0.8502 - val_loss: 0.3624 - val_acc: 0.8272\n",
      "Epoch 22/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3249 - acc: 0.8508 - val_loss: 0.3622 - val_acc: 0.8266\n",
      "Epoch 23/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3219 - acc: 0.8526 - val_loss: 0.3631 - val_acc: 0.8261\n",
      "Epoch 24/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3193 - acc: 0.8539 - val_loss: 0.3641 - val_acc: 0.8238\n",
      "Epoch 25/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3171 - acc: 0.8556 - val_loss: 0.3643 - val_acc: 0.8244\n",
      "Epoch 26/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3140 - acc: 0.8569 - val_loss: 0.3607 - val_acc: 0.8283\n",
      "Epoch 27/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3107 - acc: 0.8595 - val_loss: 0.3603 - val_acc: 0.8277\n",
      "Epoch 28/80\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3074 - acc: 0.8603 - val_loss: 0.3617 - val_acc: 0.8279\n",
      "Epoch 29/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3052 - acc: 0.8613 - val_loss: 0.3601 - val_acc: 0.8282\n",
      "Epoch 30/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3031 - acc: 0.8613 - val_loss: 0.3635 - val_acc: 0.8295\n",
      "Epoch 31/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3027 - acc: 0.8617 - val_loss: 0.3594 - val_acc: 0.8288\n",
      "Epoch 32/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2981 - acc: 0.8649 - val_loss: 0.3626 - val_acc: 0.8256\n",
      "Epoch 33/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2948 - acc: 0.8667 - val_loss: 0.3621 - val_acc: 0.8272\n",
      "Epoch 34/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2906 - acc: 0.8697 - val_loss: 0.3604 - val_acc: 0.8287\n",
      "Epoch 35/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2877 - acc: 0.8707 - val_loss: 0.3608 - val_acc: 0.8281\n",
      "Epoch 36/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2849 - acc: 0.8719 - val_loss: 0.3654 - val_acc: 0.8243\n",
      "Epoch 37/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2827 - acc: 0.8721 - val_loss: 0.3617 - val_acc: 0.8273\n",
      "Epoch 38/80\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.2790 - acc: 0.8757 - val_loss: 0.3626 - val_acc: 0.8300\n",
      "Epoch 39/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2758 - acc: 0.8774 - val_loss: 0.3640 - val_acc: 0.8267\n",
      "Epoch 40/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2728 - acc: 0.8785 - val_loss: 0.3636 - val_acc: 0.8287\n",
      "Epoch 41/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2703 - acc: 0.8800 - val_loss: 0.3635 - val_acc: 0.8277\n",
      "Epoch 42/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2669 - acc: 0.8828 - val_loss: 0.3692 - val_acc: 0.8268\n",
      "Epoch 43/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2646 - acc: 0.8837 - val_loss: 0.3658 - val_acc: 0.8285\n",
      "Epoch 44/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2616 - acc: 0.8851 - val_loss: 0.3690 - val_acc: 0.8281\n",
      "Epoch 45/80\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.2595 - acc: 0.8849 - val_loss: 0.3686 - val_acc: 0.8275\n",
      "Epoch 46/80\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.2562 - acc: 0.8871 - val_loss: 0.3718 - val_acc: 0.8279\n",
      "Epoch 47/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2532 - acc: 0.8898 - val_loss: 0.3716 - val_acc: 0.8283\n",
      "Epoch 48/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2489 - acc: 0.8921 - val_loss: 0.3734 - val_acc: 0.8272\n",
      "Epoch 49/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2460 - acc: 0.8928 - val_loss: 0.3765 - val_acc: 0.8284\n",
      "Epoch 50/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2439 - acc: 0.8948 - val_loss: 0.3761 - val_acc: 0.8252\n",
      "Epoch 51/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2397 - acc: 0.8968 - val_loss: 0.3777 - val_acc: 0.8273\n",
      "Epoch 52/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2365 - acc: 0.8985 - val_loss: 0.3795 - val_acc: 0.8260\n",
      "Epoch 53/80\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.2342 - acc: 0.9002 - val_loss: 0.3816 - val_acc: 0.8269\n",
      "Epoch 54/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2317 - acc: 0.9008 - val_loss: 0.3824 - val_acc: 0.8277\n",
      "Epoch 55/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2280 - acc: 0.9041 - val_loss: 0.3861 - val_acc: 0.8257\n",
      "Epoch 56/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2297 - acc: 0.9018 - val_loss: 0.3880 - val_acc: 0.8278\n",
      "Epoch 57/80\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.2274 - acc: 0.9030 - val_loss: 0.3864 - val_acc: 0.8278\n",
      "Epoch 58/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2220 - acc: 0.9055 - val_loss: 0.3946 - val_acc: 0.8259\n",
      "Epoch 59/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2188 - acc: 0.9080 - val_loss: 0.3897 - val_acc: 0.8273\n",
      "Epoch 60/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2143 - acc: 0.9103 - val_loss: 0.3928 - val_acc: 0.8269\n",
      "Epoch 61/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2125 - acc: 0.9112 - val_loss: 0.3960 - val_acc: 0.8263\n",
      "Epoch 62/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2096 - acc: 0.9123 - val_loss: 0.4001 - val_acc: 0.8252\n",
      "Epoch 63/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2078 - acc: 0.9143 - val_loss: 0.3987 - val_acc: 0.8252\n",
      "Epoch 64/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2035 - acc: 0.9165 - val_loss: 0.4018 - val_acc: 0.8267\n",
      "Epoch 65/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2007 - acc: 0.9182 - val_loss: 0.4081 - val_acc: 0.8271\n",
      "Epoch 66/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.1996 - acc: 0.9182 - val_loss: 0.4066 - val_acc: 0.8256\n",
      "Epoch 67/80\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.1970 - acc: 0.9199 - val_loss: 0.4127 - val_acc: 0.8262\n",
      "Epoch 68/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.1940 - acc: 0.9211 - val_loss: 0.4111 - val_acc: 0.8248\n",
      "Epoch 69/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.1912 - acc: 0.9229 - val_loss: 0.4165 - val_acc: 0.8267\n",
      "Epoch 70/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.1892 - acc: 0.9240 - val_loss: 0.4188 - val_acc: 0.8249\n",
      "Epoch 71/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.1863 - acc: 0.9253 - val_loss: 0.4191 - val_acc: 0.8244\n",
      "Epoch 72/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.1837 - acc: 0.9267 - val_loss: 0.4221 - val_acc: 0.8268\n",
      "Epoch 73/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.1811 - acc: 0.9286 - val_loss: 0.4244 - val_acc: 0.8251\n",
      "Epoch 74/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.1791 - acc: 0.9286 - val_loss: 0.4270 - val_acc: 0.8253\n",
      "Epoch 75/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.1761 - acc: 0.9311 - val_loss: 0.4294 - val_acc: 0.8263\n",
      "Epoch 76/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.1739 - acc: 0.9321 - val_loss: 0.4332 - val_acc: 0.8239\n",
      "Epoch 77/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.1719 - acc: 0.9322 - val_loss: 0.4362 - val_acc: 0.8228\n",
      "Epoch 78/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.1706 - acc: 0.9334 - val_loss: 0.4395 - val_acc: 0.8237\n",
      "Epoch 79/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.1671 - acc: 0.9352 - val_loss: 0.4430 - val_acc: 0.8261\n",
      "Epoch 80/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.1662 - acc: 0.9343 - val_loss: 0.4446 - val_acc: 0.8231\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=80,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readable_output(output):\n",
    "    intensity = output[0][:3]\n",
    "    polarity = output[0][3:]\n",
    "    pol_str = \"\"\n",
    "    int_str = \"\"\n",
    "    if max(intensity) == intensity[0]:\n",
    "        int_str = \"high\"\n",
    "    elif max(intensity) == intensity[1]:\n",
    "        int_str = \"med\"\n",
    "    else:\n",
    "        int_str = \"low\"\n",
    "    if max(polarity) == polarity[0]:\n",
    "        pol_str = \"pos\"\n",
    "    elif max(polarity) == polarity[1]:\n",
    "        pol_str = \"neutral\"\n",
    "    else:\n",
    "        pol_str = \"neg\"\n",
    "    return \"{} {}\".format(int_str, pol_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9006553  0.01879829 0.02905652 0.32951632 0.00907019 0.57859606]]\n",
      "I really hate this \n",
      " high neg \n",
      "\n",
      "[[0.9238026  0.05397011 0.02437352 0.41188955 0.00351907 0.7160095 ]]\n",
      "I don't like this \n",
      " high neg \n",
      "\n",
      "[[0.89268696 0.01106873 0.04471983 0.81357646 0.00932928 0.1418184 ]]\n",
      "I think this is nice \n",
      " high pos \n",
      "\n",
      "[[0.5061053  0.05087366 0.23811758 0.8225473  0.02424105 0.0759644 ]]\n",
      "I love this thing so much \n",
      " high pos \n",
      "\n",
      "[[0.0276257  0.00144556 0.97583145 0.00902619 0.79991555 0.1624663 ]]\n",
      "I feel relatively indifferent \n",
      " low neutral \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"I really hate this\",\n",
    "    \"I don't like this\",\n",
    "    \"I think this is nice\",\n",
    "    \"I love this thing so much\",\n",
    "    \"I feel relatively indifferent\",]\n",
    "for doc in docs:\n",
    "    result = model.predict(np.asmatrix(nlp(doc).vector))\n",
    "    print(result)\n",
    "    print(doc, \"\\n\", readable_output(result), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpler Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(300, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(32, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(2, activation=tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = np.stack(training_df['vector'].values, axis=0)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data = training_df[['intensity', 'polarity']].values\n",
    "label_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.2\n",
    "cutoff = int(len(input_data) * split_frac)\n",
    "x_val = input_data[:cutoff]\n",
    "x_train = input_data[cutoff:]\n",
    "\n",
    "y_val = label_data[:cutoff]\n",
    "y_train = label_data[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/80\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.3743 - mean_absolute_error: 0.1450 - val_loss: 0.6916 - val_mean_absolute_error: 0.2477\n",
      "Epoch 2/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3377 - mean_absolute_error: 0.1289 - val_loss: 0.6568 - val_mean_absolute_error: 0.2448\n",
      "Epoch 3/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3189 - mean_absolute_error: 0.1149 - val_loss: 0.6429 - val_mean_absolute_error: 0.2372\n",
      "Epoch 4/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3101 - mean_absolute_error: 0.1086 - val_loss: 0.6543 - val_mean_absolute_error: 0.2392\n",
      "Epoch 5/80\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3077 - mean_absolute_error: 0.1068 - val_loss: 0.6522 - val_mean_absolute_error: 0.2386\n",
      "Epoch 6/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3050 - mean_absolute_error: 0.1045 - val_loss: 0.6526 - val_mean_absolute_error: 0.2381\n",
      "Epoch 7/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3032 - mean_absolute_error: 0.1035 - val_loss: 0.6546 - val_mean_absolute_error: 0.2382\n",
      "Epoch 8/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3031 - mean_absolute_error: 0.1034 - val_loss: 0.6559 - val_mean_absolute_error: 0.2375\n",
      "Epoch 9/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3023 - mean_absolute_error: 0.1030 - val_loss: 0.6579 - val_mean_absolute_error: 0.2381\n",
      "Epoch 10/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3007 - mean_absolute_error: 0.1017 - val_loss: 0.6578 - val_mean_absolute_error: 0.2380\n",
      "Epoch 11/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2992 - mean_absolute_error: 0.1005 - val_loss: 0.6623 - val_mean_absolute_error: 0.2385\n",
      "Epoch 12/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2987 - mean_absolute_error: 0.1001 - val_loss: 0.6637 - val_mean_absolute_error: 0.2383\n",
      "Epoch 13/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2975 - mean_absolute_error: 0.0995 - val_loss: 0.6663 - val_mean_absolute_error: 0.2389\n",
      "Epoch 14/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2974 - mean_absolute_error: 0.0994 - val_loss: 0.6649 - val_mean_absolute_error: 0.2381\n",
      "Epoch 15/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2957 - mean_absolute_error: 0.0982 - val_loss: 0.6670 - val_mean_absolute_error: 0.2389\n",
      "Epoch 16/80\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.2942 - mean_absolute_error: 0.0970 - val_loss: 0.6685 - val_mean_absolute_error: 0.2379\n",
      "Epoch 17/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2936 - mean_absolute_error: 0.0965 - val_loss: 0.6718 - val_mean_absolute_error: 0.2392\n",
      "Epoch 18/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2923 - mean_absolute_error: 0.0954 - val_loss: 0.6723 - val_mean_absolute_error: 0.2385\n",
      "Epoch 19/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2911 - mean_absolute_error: 0.0948 - val_loss: 0.6740 - val_mean_absolute_error: 0.2388\n",
      "Epoch 20/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2904 - mean_absolute_error: 0.0938 - val_loss: 0.6778 - val_mean_absolute_error: 0.2383\n",
      "Epoch 21/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2902 - mean_absolute_error: 0.0941 - val_loss: 0.6787 - val_mean_absolute_error: 0.2396\n",
      "Epoch 22/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2889 - mean_absolute_error: 0.0930 - val_loss: 0.6811 - val_mean_absolute_error: 0.2387\n",
      "Epoch 23/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2873 - mean_absolute_error: 0.0922 - val_loss: 0.6816 - val_mean_absolute_error: 0.2390\n",
      "Epoch 24/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2860 - mean_absolute_error: 0.0909 - val_loss: 0.6862 - val_mean_absolute_error: 0.2385\n",
      "Epoch 25/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2855 - mean_absolute_error: 0.0907 - val_loss: 0.6862 - val_mean_absolute_error: 0.2386\n",
      "Epoch 26/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2853 - mean_absolute_error: 0.0901 - val_loss: 0.6868 - val_mean_absolute_error: 0.2379\n",
      "Epoch 27/80\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.2837 - mean_absolute_error: 0.0895 - val_loss: 0.6896 - val_mean_absolute_error: 0.2392\n",
      "Epoch 28/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2824 - mean_absolute_error: 0.0880 - val_loss: 0.6936 - val_mean_absolute_error: 0.2390\n",
      "Epoch 29/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2810 - mean_absolute_error: 0.0871 - val_loss: 0.6964 - val_mean_absolute_error: 0.2386\n",
      "Epoch 30/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2799 - mean_absolute_error: 0.0863 - val_loss: 0.6987 - val_mean_absolute_error: 0.2395\n",
      "Epoch 31/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2787 - mean_absolute_error: 0.0854 - val_loss: 0.6996 - val_mean_absolute_error: 0.2377\n",
      "Epoch 32/80\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.2793 - mean_absolute_error: 0.0862 - val_loss: 0.7026 - val_mean_absolute_error: 0.2394\n",
      "Epoch 33/80\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.2774 - mean_absolute_error: 0.0845 - val_loss: 0.7063 - val_mean_absolute_error: 0.2394\n",
      "Epoch 34/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2758 - mean_absolute_error: 0.0838 - val_loss: 0.7063 - val_mean_absolute_error: 0.2382\n",
      "Epoch 35/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2752 - mean_absolute_error: 0.0826 - val_loss: 0.7110 - val_mean_absolute_error: 0.2401\n",
      "Epoch 36/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2748 - mean_absolute_error: 0.0824 - val_loss: 0.7090 - val_mean_absolute_error: 0.2381\n",
      "Epoch 37/80\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.2727 - mean_absolute_error: 0.0809 - val_loss: 0.7186 - val_mean_absolute_error: 0.2408\n",
      "Epoch 38/80\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.2724 - mean_absolute_error: 0.0805 - val_loss: 0.7157 - val_mean_absolute_error: 0.2382\n",
      "Epoch 39/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2716 - mean_absolute_error: 0.0799 - val_loss: 0.7213 - val_mean_absolute_error: 0.2403\n",
      "Epoch 40/80\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.2699 - mean_absolute_error: 0.0789 - val_loss: 0.7213 - val_mean_absolute_error: 0.2394\n",
      "Epoch 41/80\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.2694 - mean_absolute_error: 0.0780 - val_loss: 0.7281 - val_mean_absolute_error: 0.2386\n",
      "Epoch 42/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2692 - mean_absolute_error: 0.0785 - val_loss: 0.7329 - val_mean_absolute_error: 0.2422\n",
      "Epoch 43/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2695 - mean_absolute_error: 0.0789 - val_loss: 0.7321 - val_mean_absolute_error: 0.2393\n",
      "Epoch 44/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2679 - mean_absolute_error: 0.0777 - val_loss: 0.7389 - val_mean_absolute_error: 0.2429\n",
      "Epoch 45/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2670 - mean_absolute_error: 0.0766 - val_loss: 0.7332 - val_mean_absolute_error: 0.2400\n",
      "Epoch 46/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2641 - mean_absolute_error: 0.0744 - val_loss: 0.7328 - val_mean_absolute_error: 0.2386\n",
      "Epoch 47/80\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.2637 - mean_absolute_error: 0.0740 - val_loss: 0.7441 - val_mean_absolute_error: 0.2414\n",
      "Epoch 48/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2636 - mean_absolute_error: 0.0739 - val_loss: 0.7405 - val_mean_absolute_error: 0.2380\n",
      "Epoch 49/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2636 - mean_absolute_error: 0.0732 - val_loss: 0.7465 - val_mean_absolute_error: 0.2412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2622 - mean_absolute_error: 0.0722 - val_loss: 0.7449 - val_mean_absolute_error: 0.2391\n",
      "Epoch 51/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2606 - mean_absolute_error: 0.0718 - val_loss: 0.7503 - val_mean_absolute_error: 0.2414\n",
      "Epoch 52/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2611 - mean_absolute_error: 0.0722 - val_loss: 0.7525 - val_mean_absolute_error: 0.2398\n",
      "Epoch 53/80\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.2616 - mean_absolute_error: 0.0720 - val_loss: 0.7626 - val_mean_absolute_error: 0.2418\n",
      "Epoch 54/80\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.2604 - mean_absolute_error: 0.0716 - val_loss: 0.7570 - val_mean_absolute_error: 0.2408\n",
      "Epoch 55/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2590 - mean_absolute_error: 0.0701 - val_loss: 0.7613 - val_mean_absolute_error: 0.2405\n",
      "Epoch 56/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2566 - mean_absolute_error: 0.0683 - val_loss: 0.7602 - val_mean_absolute_error: 0.2395\n",
      "Epoch 57/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2565 - mean_absolute_error: 0.0686 - val_loss: 0.7643 - val_mean_absolute_error: 0.2401\n",
      "Epoch 58/80\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.2566 - mean_absolute_error: 0.0685 - val_loss: 0.7655 - val_mean_absolute_error: 0.2403\n",
      "Epoch 59/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2554 - mean_absolute_error: 0.0672 - val_loss: 0.7676 - val_mean_absolute_error: 0.2381\n",
      "Epoch 60/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2528 - mean_absolute_error: 0.0652 - val_loss: 0.7733 - val_mean_absolute_error: 0.2415\n",
      "Epoch 61/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2517 - mean_absolute_error: 0.0643 - val_loss: 0.7742 - val_mean_absolute_error: 0.2390\n",
      "Epoch 62/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2518 - mean_absolute_error: 0.0644 - val_loss: 0.7768 - val_mean_absolute_error: 0.2400\n",
      "Epoch 63/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2499 - mean_absolute_error: 0.0630 - val_loss: 0.7793 - val_mean_absolute_error: 0.2399\n",
      "Epoch 64/80\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.2493 - mean_absolute_error: 0.0623 - val_loss: 0.7839 - val_mean_absolute_error: 0.2402\n",
      "Epoch 65/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2490 - mean_absolute_error: 0.0619 - val_loss: 0.7845 - val_mean_absolute_error: 0.2404\n",
      "Epoch 66/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2508 - mean_absolute_error: 0.0624 - val_loss: 0.7905 - val_mean_absolute_error: 0.2422\n",
      "Epoch 67/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2506 - mean_absolute_error: 0.0627 - val_loss: 0.7923 - val_mean_absolute_error: 0.2392\n",
      "Epoch 68/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2491 - mean_absolute_error: 0.0615 - val_loss: 0.7966 - val_mean_absolute_error: 0.2409\n",
      "Epoch 69/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2478 - mean_absolute_error: 0.0616 - val_loss: 0.7919 - val_mean_absolute_error: 0.2395\n",
      "Epoch 70/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2471 - mean_absolute_error: 0.0603 - val_loss: 0.7945 - val_mean_absolute_error: 0.2399\n",
      "Epoch 71/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2450 - mean_absolute_error: 0.0589 - val_loss: 0.8010 - val_mean_absolute_error: 0.2398\n",
      "Epoch 72/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2448 - mean_absolute_error: 0.0589 - val_loss: 0.7991 - val_mean_absolute_error: 0.2393\n",
      "Epoch 73/80\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.2432 - mean_absolute_error: 0.0573 - val_loss: 0.8049 - val_mean_absolute_error: 0.2404\n",
      "Epoch 74/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2425 - mean_absolute_error: 0.0567 - val_loss: 0.8090 - val_mean_absolute_error: 0.2411\n",
      "Epoch 75/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2425 - mean_absolute_error: 0.0569 - val_loss: 0.8072 - val_mean_absolute_error: 0.2392\n",
      "Epoch 76/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2423 - mean_absolute_error: 0.0567 - val_loss: 0.8129 - val_mean_absolute_error: 0.2420\n",
      "Epoch 77/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2425 - mean_absolute_error: 0.0573 - val_loss: 0.8106 - val_mean_absolute_error: 0.2393\n",
      "Epoch 78/80\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.2414 - mean_absolute_error: 0.0564 - val_loss: 0.8223 - val_mean_absolute_error: 0.2408\n",
      "Epoch 79/80\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.2403 - mean_absolute_error: 0.0551 - val_loss: 0.8192 - val_mean_absolute_error: 0.2402\n",
      "Epoch 80/80\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.2405 - mean_absolute_error: 0.0549 - val_loss: 0.8207 - val_mean_absolute_error: 0.2405\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=80,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15353452 0.02435592]]\n",
      "[[0.07771567 0.00667944]]\n",
      "[[0.5506569 0.7326522]]\n",
      "[[0.00711581 0.6554239 ]]\n",
      "[[0.00308359 0.8789249 ]]\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"I really hate this\",\n",
    "    \"I don't like this\",\n",
    "    \"I think this is nice\",\n",
    "    \"I love this thing so much\",\n",
    "    \"I feel relatively indifferent\",]\n",
    "for doc in docs:\n",
    "    result = model.predict(np.asmatrix(nlp(doc).vector))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
